#+Title: Logic and Proof
#+Author: [[http://www.andrew.cmu.edu/user/avigad][Jeremy Avigad]], [[http://www.andrew.cmu.edu/user/rlewis1/][Robert Y. Lewis]],  [[http://www.contrib.andrew.cmu.edu/~fpv/][Floris van Doorn]]

* The Existential Quantifier

As discussed in Chapter [[file:04_First_Order_Logic.org::#First_Order_Logic][First Order Logic]], there are two quantifiers
in first-order logic. We have discussed the rules for reasoning about
the universal quantifier, so now we turn to the rules of the
existential quantifier.

** Rules for the Existential Quantifier

The following statements about the natural numbers refer to the existence of some natural number:

- There exists an odd composite number.  (Remember that a natural number is
  /composite/ if it is greater than 1 and not prime.)
- Every natural number greater than one has a prime divisor.
- For every $n$, if $n$ has a prime divisor smaller than $n$, then $n$ is composite.

These statements can be expressed in first-order logic using the existential quantifier in the
following ways:

- $\ex n (\fn{odd}(n) \wedge \fn{composite}(n))$
- $\fa n (n > 1 \to \ex p ( \fn{prime}(p) \wedge p\mid n))$
- $\fa n ((\ex p (p \mid n \wedge \fn{prime}(p) \wedge p < n)) \to \fn{composite}(n))$

After we write $\exists \; n$, the variable $n$ is /bound/ in the
formula, just as for the universal quantifier. So the formulas $\ex n
\fn{composite}(n)$ and $\ex m \fn{composite}(m)$ are considered the
same.

How do we prove such existential statements? Suppose we want to prove that there exists an odd
composite number. To do this, we just present a candidate, and show that the candidate satisfies the
required properties. For example, we could choose 15, and then show that 15 is odd and that 15 is
prime. Of course, there's nothing special about 15, and we could have proven it also using a
different number, like 9 or 35. The choice of candidate does not
matter, as long as it has the required property.

In a natural deduction proof this would look like this:
\begin{prooftree}
\AXM{\vdots}
\UIM{\fn{odd}(15)\wedge\fn{composite}(15)}
\UIM{\ex n(\fn{odd}(n)\wedge\fn{composite}(n))}
\end{prooftree}

This illustrates the introduction rule for the existential quantifier:
\begin{prooftree}
\AXM{A(t)}
\UIM{\ex x A(x)}
\end{prooftree}
where $t$ is any term. So to prove an existential formula, we just have to give one particular
term for which we can prove that formula. Such term is called a /witness/ for the formula.

What about the elimination rule? Suppose that we know that $n$ is some natural number and we know
that there exists a prime $p$ such that $p < n$ and $p \mid n$. How can we use this to prove that
$n$ is composite? We can reason as follows:
#+BEGIN_QUOTE
Let $p$ be any prime such that $p < n$ and $p \mid n$. 

...

Therefore, $n$ is composite.
#+END_QUOTE

First, we assume that there is some $p$ which satisfies the properties
$p$ is prime, $p < n$ and $p \mid n$, and then we reason about that
$p$. As with case-based reasoning using "or," the assumption is only
temporary: if we can show that $n$ is composite from that assumption,
that we have essentially shown that $n$ is composite assuming the
existence of such a $p$. Notice that in this pattern of reasoning, $p$
should be "arbitrary." In other words, we should not have assumed
anything about $p$ beforehand, we should not make any additional
assumptions about $p$ along the way, and the conclusion should not
mention $p$. Only then does it makes sense to say that the conclusion
follows from the "mere" existence of a $p$ with the assumed
properties.

In natural deduction, the elimination rule is expressed as follows:
\begin{prooftree}
\AXM{\ex x A(x)}
\AXM{}
\RLM{a}
\UIM{A(y)}
\UIM{\vdots}
\UIM{B}
\RLM{a}
\BIM{B}
\end{prooftree}
Here we require that $y$ is not free in $B$, and that the only
uncanceled hypotheses where $y$ occurs freely are the hypotheses
$A(y)$ that are canceled when you apply this rule. Formally, this is
what it means to say that $y$ is "arbitrary." As was the case for or
elimination and implication introduction, you can use the hypothesis
$A(y)$ multiple times in the proof of $B$, and cancel all of them at
once.

Intuitively, the rule says that you can prove $B$ from the assumption
$\ex x A(x)$ by assuming $A(y)$ for a fresh variable $y$, and
concluding, in any number of steps, that $B$ follows. You should
compare this rule to the rule for or elimination, which is somewhat
analogous. In the following example, we show that if $A(x)$ always
implies $\neg B(x)$, then there cannot be an $x$ for which both $A(x)$
and $B(x)$ holds.

\begin{prooftree}
\AXM{}
\RLM{b}
\UIM{\ex x(A(x) \wedge B(x))}
\AXM{}
\RLM{a}
\UIM{\fa x (A(x) \to \neg B(x))}
\UIM{A(x) \to \neg B(x)}
\AXM{}
\RLM{c}
\UIM{A(x) \wedge B(x)}
\UIM{A(x)}
\BIM{\neg B(x)}
\AXM{}
\RLM{c}
\UIM{A(x) \wedge B(x)}
\UIM{B(x)}
\BIM{\bot}
\RLM{c}
\BIM{\bot}
\RLM{b}
\UIM{\neg\ex x(A(x) \wedge B(x))}
\RLM{a}
\UIM{\fa x (A(x) \to \neg B(x)) \to \neg\ex x(A(x) \wedge B(x))}
\end{prooftree}
In this proof tree, the existential elimination rule (the line labeled
$c$) is used to cancel two hypotheses at the same time. Note that when
this rule is applied, the hypothesis $\fa x (A(x) \to \neg B(x))$ has
not yet been canceled. So we have to make sure that this formula
doesn't contain the variable $x$ freely. But this is o.k., since this
hypothesis contains $x$ only as a bound variable.

Another example is that if $x$ does not occur in $P$, then $\ex x P$ is equivalent to $P$:
\begin{prooftree}
\AXM{}
\RLM{a}
\UIM{\ex x P}
\AXM{}
\RLM{b}
\UIM{P}
\RLM{b}
\BIM{P}
\AXM{}
\RLM{a}
\UIM{P}
\UIM{\ex x P}
\RLM{a}
\BIM{\ex x P \liff P}
\end{prooftree}
This short but tricky, so let us go through it carefully. On the left, we
assume $\ex x P$ to conclude $P$. We assume $P$, and now we can
immediately cancel this assumption by existential elimination, since
$x$ does not occur in $P$, so it doesn't occur freely in any
assumption or in the conclusion. On the right we use existential
introduction to conclude $\ex x P$ from $P$.

We can also reason with existential quantification in Lean. We can enter the existential quantifier
with =\ex=. The introduction rule is =exists.intro=. This requires two arguments: a term, and a
proof that that term satisfies the required property.

#+BEGIN_SRC lean
variable U : Type
variable P : U → Prop

example (y : U) (H : P y) : ∃x, P x :=
exists.intro y H
#+END_SRC
The elimination rule for the existential quantifier is given by the =obtain= command. 
Given a term of type =∃x, P x= we can use it to get a new variable =y= and a proof that =P y= holds.
#+BEGIN_SRC lean
variable U : Type
variable P : U → Prop
variable Q : Prop

example (H1 : ∃x, P x) (H2 : ∀x, P x → Q) : Q :=
obtain (y : U) (H : P y), from H1,
have P y → Q, from H2 y,
show Q, from this H
#+END_SRC
Some additional notes:
- We can use =obtain= without specifying the type of the variables and
  proofs we get. So if we write =obtain y H= instead of =obtain (y : U)
  (H : P y)= in the first line of the previous proof, that is also
  accepted.
- We can also introduce anonymous variables, giving the type between
  backticks instead of the variable. Then we can later refer to this
  assertion by writing the type between backticks again. It works the
  same as a anonymous =have= expression. However, we cannot use the
  keyword =this= for variables introduced by =obtain=.
- We can also use =obtain= to destruct a hypothesis of the form =P ∧
  Q= to get a proof of =P= and a proof of =Q=.

These features are all illustrated in the following example:
#+BEGIN_SRC lean
variable U : Type
variables P R : U → Prop
variable Q : Prop

example (H1 : ∃x, P x ∧ R x) (H2 : ∀x, P x → R x → Q) : Q :=
obtain y `P y` `R y`, from H1,
show Q, from H2 y `P y` `R y`
#+END_SRC

The proof trees we gave above can be formulated in Lean as follows. In this example we also use the
anonymous =assume=, which works the same as the anonymous =obtain=.

#+BEGIN_SRC lean
variable U : Type
variable u : U
variables A B : U → Prop
variable P : Prop

example : (∀x, A x → ¬ B x) → ¬ ∃x, A x ∧ B x :=
assume `∀x, A x → ¬ B x` `∃x, A x ∧ B x`,
obtain x `A x` `B x`, from `∃x, A x ∧ B x`,
have ¬ B x, from `∀x, A x → ¬ B x` x `A x`,
show false, from `¬ B x` `B x`

example : (∃x : U, P) ↔ P :=
iff.intro
  (assume H : ∃x, P, 
   obtain x `P`, from H,
   `P`)
  (assume `P`, exists.intro u `P`)
#+END_SRC

In the second example we use that =U= is inhabited by the element =u=,
because the statement is false for nonempty =U=. In the natural
deduction proof we didn't have to require this specifically. This is
because we implicitly assume that the underlying universe we work in
is nonempty. We will elaborate on this in Chapter [[file:07_Semantics_of_First_Order_Logic.org::#Semantics_of_First_Order_Logic][Semantics of First
Order Logic]].

** Counterexamples and Relativized Quantifiers

Consider the statement:
#+BEGIN_QUOTE
Every prime number is odd.
#+END_QUOTE
In first-order logic, we could formulate this as $\fa p (\fn{prime}(p) \to \fn{odd}(p))$. This
statement is false, because there is a prime number which is even, namely the number 2. This is
called a /counterexample/ to the statement. 

More generally, given a formula $\fa x A(x)$, a counterexample is a value $t$ such that $\neg A(t)$
holds. Such a counterexample shows that the original formula is false. The reason for this is the
following equivalence: $\neg\fa x A(x) \liff \ex x \neg A(x)$. So if we find a value $t$ such that
$\neg A(t)$ holds, then by the existential introduction rule we can conclude that $\ex x \neg A(x)$,
and then by the above equivalence we have $\neg\fa x A(x)$. We now give the proof of this
equivalence:

\begin{prooftree}
\AXM{}
\RLM{a}
\UIM{\neg\fa x A(x)}
\AXM{}
\RLM{d}
\UIM{\neg(\ex x \neg A(x))}
\AXM{}
\RLM{e}
\UIM{\neg A(x)}
\UIM{\ex x \neg A(x)}
\BIM{\bot}
\RLM{e}
\UIM{A(x)}
\UIM{\fa x A(x)}
\BIM{}
\RLM{d}
\UIM{\ex x \neg A(x)}
\AXM{}
\RLM{a}
\UIM{\ex x \neg A(x)}
\AXM{}
\RLM{c}
\UIM{\neg A(y)}
\AXM{}
\RLM{b}
\UIM{\fa x A(x)}
\UIM{A(y)}
\BIM{\bot}
\RLM{c}
\BIM{\bot}
\RLM{b}
\UIM{\neg\fa x A(x)}
\RLM{a}
\BIM{\neg\fa x A(x) \liff \ex x \neg A(x)}
\end{prooftree}

One remark about the proof: at the step marked by $d$ we /cannot/ use
the existential introduction rule, because at that point our only
assumption is $\neg\fa x A(x)$, and from that assumption we cannot
prove $\neg A(t)$ for a particular term $t$. So we use a proof by
contradiction there. 

As an exercise, prove the "dual" equivalence yourself: $\neg\ex x A(x)
\liff \fa x \neg A(x)$. This can be done without using proof by
contradiction.

In Chapter [[file:04_First_Order_Logic.org::#First_Order_Logic][First Order Logic]] we saw examples how to use relativization
to restrict the scope of a universal quantifier. Suppose we want to
say "every prime number is greater than 1". In first order logic this
can be written as $\fa n (\fn{prime}(n) \to n > 1)$. The reason is
that the original statement is equivalent to the statement "for every
natural number, if it is prime, then it is greater than 1".

We can do the same for an existential quantifier. Suppose we want to
write "there exists a prime number greater than 100" in first-order
logic. We can alternatively write this as "there exists a natural
number which is prime and greater than 100." This makes the
translation into first order logic easy: $\ex n(\fn{prime}(n) \wedge n
> 100)$. Notice what is happening here: if we relativize the universal
quantifier we add an implication in the formula. However, if we
relativize the existential quantifier, we add a conjunction.

As an exercise you can prove the above results about negations of
quantifiers also for relativized quantifiers. Specifically, prove the
following statements:
- $\neg\ex x (A(x) \wedge B(x)) \liff \fa x ( A(x) \to \neg B(x))$;
- $\neg\fa x (A(x) \to B(x)) \liff \ex x ( A(x) \wedge \neg B(x))$

In first-order logic there are only two quantifiers: the universal
quantifier and the existential quantifier. However, we already saw
that using these we can /define/ many more quantifiers, namely all
relativized quantifiers. But we can define even more quantifiers.

- We can define "there are at least two elements $x$ such that $A(x)$ holds." We can write this as
  $\ex x \ex y (x \neq y \wedge A(x) \wedge A(y))$.
- We can define "there are at most two elements $x$ such that $A(x)$ holds." We can write this as
  $\fa x \fa y \fa z (A(x) \wedge A(y) \wedge A(z) \to x = y \vee y = z \vee x = z)$. This states
  that if we have three elements $a$ for which $A(a)$ holds, then two of them must be equal.
- We can define "there are exactly two elements $x$ such that $A(x)$ holds" as the conjunction of
  the above two statements.

As an exercise, write out in first order logic the statements that there are at least, at most, and
exactly three elements $x$ such that $A(x)$ holds.

For reference, here is a list of valid sentences involving quantifiers:
- $\fa x A \liff A$ if $x$ is not free in $A$
- $\ex x A \liff A$ if $x$ is not free in $A$
- $\fa x (A(x) \land B(x)) \liff \fa x A(x) \land \fa x B(x)$
- $\ex x (A(x) \land B) \liff \ex x A(x) \land B$ if $x$ is
  not free in $B$
- $\ex x (A(x) \lor B(x)) \liff \ex x A(x) \lor \ex x B(x)$
- $\fa x (A(x) \lor B) \liff \fa x A(x) \lor B$ if $x$ is not
  free in $B$
- $\fa x (A(x) \to B) \liff (\ex x A(x) \to B)$ if
  $x$ is not free in $B$
- $\ex x (A(x) \to B) \liff (\fa x A(x) \to
  B)$ if $x$ is not free in $B$
- $\fa x (A \to B(x)) \liff (A \to \fa x B(x))$ if
  $x$ is not free in $A$
- $\ex x (A(x) \to B) \liff (A(x) \to \ex x B)$
  if $x$ is not free in $B$
- $\ex x A(x) \liff \neg \fa x \neg A(x)$
- $\fa x A(x) \liff \neg \ex x \neg A(x)$
- $\neg \ex x A(x) \liff \fa x \neg A(x)$
- $\neg \fa x A(x) \liff \ex x \neg A(x)$


** Divisibility

Using the existential quantifier, we can now define divisibility on the natural numbers.
-----
*Definition*. Given two natural numbers $m$ and $n$. We say that $m$ /is a divisor of/ $n$, written
 $m \mid n$, if there exists some $k$ such that $m \cdot k = n$. We also say that $n$ /is divisible by/
 $m$ or that $m$ /divides/ $n$.
-----

We can now prove the following:

-----
*Theorem.* The relation $\mid$ is a partial order. However, it is not total.

*Proof.* We have to show that $\mid$ is reflexive, antisymmetric and transitive. Reflexivity is
immediate, because $n \cdot 1 = n$, hence $n\mid n$.

For antisymmetry, suppose that $n$ and $m$ are natural numbers such that $n\mid m$ and $m \mid
n$. Then there exist $k$ and $\ell$ such that $n\cdot k = m$ and $m \cdot \ell = n$. We distinguish
two cases. If $n = 0$, then we have $m = n\cdot k = 0 = n$, so we are done. If $n > 0$, then we use
the the equations to get $n \cdot k \cdot \ell = m \cdot \ell = n$, and we can cancel $n$ on both
sides to get $k \cdot \ell = 1$. The product of two natural numbers can only be 1 if both natural
numbers is 1, so we conclude that $k = \ell = 1$. hence we get $n = n \cdot k = m$, so $\mid$ is
antisymmetric.

Finally, to prove transitivity, suppose $m \mid n$ and $n \mid r$. Then there are $k,\ell$ such that
$m \cdot k = n$ and $n \cdot \ell = r$. Now we compute
\begin{align*}
m \cdot (k \cdot \ell) &= (m \cdot k) \cdot \ell \\
& = n \cdot \ell  \\
& = r.
\end{align*}
-----

The proof can also be carried out in Lean. For clarity we break the
proof into steps. Here is reflexivity:
#+BEGIN_SRC lean
import data.nat
open nat

example : ∀n : ℕ, n ∣ n :=
take n,
have n = n * 1, from eq.symm (mul_one n),
show n ∣ n, from exists.intro 1 `n = n * 1`
#+END_SRC
In Lean, you have to input $\mid$ as =\|=. The character is not the
same as a regular vertical bar =|=.  Note also that, in Lean, $m \mid
n$ in Lean is defined as $\ex k (n = m * k)$ instead of $\ex k (m * k
= n)$, and we have to be sure that we prove the condition in exactly
the right form.

Here is antisymmetry:
#+BEGIN_SRC lean
import data.nat
open nat

-- BEGIN
example : ∀ n m : ℕ, m ∣ n → n ∣ m → n = m :=
take m n,
assume `n ∣ m` `m ∣ n`,
obtain k `m = n * k`, from `n ∣ m`,
obtain l `n = m * l`, from `m ∣ n`,
or.elim (eq_zero_or_pos n)
  (assume `n = 0`,
   show m = n, from calc
       m = n * k : `m = n * k`
     ... = 0 * k : {`n = 0`}
     ... = 0     : zero_mul
     ... = n     : `n = 0`)
  (assume `n > 0`,
    have n * 1 = n * (k * l), from calc
      n * 1 = n         : mul_one
        ... = m * l       : `n = m * l`
        ... = (n * k) * l : {`m = n * k`}
        ... = n * (k * l) : mul.assoc,
    have 1 = k * l, 
      from eq_of_mul_eq_mul_left `n > 0` `n * 1 = n * (k * l)`,
    have k = 1, 
      from eq_one_of_mul_eq_one_right (eq.symm `1 = k * l`),
    show m = n, from calc
        m = n * k : `m = n * k`
      ... = n * 1 : {`k = 1`}
      ... = n     : mul_one)
-- END
#+END_SRC
The proof is considerably longer, but it follows the informal argument
quite closely. In some calculation steps we have written ={ `...` }=
where =...= is the equality we are rewriting (and which we have
previously assumed or proven). The curly brackets indicate that we
rewrite a subterm, and not the whole expression. For example, if we
carry out the step =m = n=, we can give the justification =`m = n`=,
but if we rewrite =m + k = n + k= we have to give the justification
={`m = n`}=. You can alternatively use labels instead of backticks,
as usual.

Finally, here is transitivity:
#+BEGIN_SRC lean
import data.nat
open nat

-- BEGIN
example : ∀ n m r : ℕ, m ∣ n → n ∣ r → m ∣ r :=  
take n m r,
assume `m ∣ n` `n ∣ r`,
obtain k `n = m * k`, from `m ∣ n`,
obtain l `r = n * l`, from `n ∣ r`,
have r = m * (k * l), from calc
    r = n * l       : `r = n * l`
  ... = (m * k) * l : {`n = m * k`}
  ... = m * (k * l) : mul.assoc,
exists.intro (k * l) `r = m * (k * l)`
-- END
#+END_SRC

# Here is everything combined as a single proof:
#
# #+BEGIN_SRC lean
# import data.nat
# open nat

# example : (∀n, n ∣ n) ∧ (∀n m, m ∣ n → n ∣ m → n = m) ∧ (∀n m r, m ∣ n → n ∣ r → m ∣ r) :=
# have refl : ∀n, n ∣ n, from
#   take n,
#   have n = n * 1, from eq.symm (mul_one n),
#   show n ∣ n, from exists.intro 1 `n = n * 1`,
# have antisymm : ∀n m, m ∣ n → n ∣ m → n = m, from
#   take m n,
#   assume `n ∣ m` `m ∣ n`,
#   obtain k `m = n * k`, from `n ∣ m`,
#   obtain l `n = m * l`, from `m ∣ n`,
#   or.elim (eq_zero_or_pos n)
#    (assume `n = 0`,
#     show m = n, from calc
#         m = n * k : `m = n * k`
#       ... = 0 * k : {`n = 0`}
#       ... = 0     : zero_mul
#       ... = n     : `n = 0`)
#    (assume `n > 0`,
#     have n * 1 = n * (k * l), from calc
#       n * 1 = n         : mul_one
#       ... = m * l       : `n = m * l`
#       ... = (n * k) * l : {`m = n * k`}
#       ... = n * (k * l) : mul.assoc,
#     have 1 = k * l, from eq_of_mul_eq_mul_left `n > 0` `n * 1 = n * (k * l)`,
#     have k = 1, from eq_one_of_mul_eq_one_right (eq.symm `1 = k * l`),
#     show m = n, from calc
#         m = n * k : `m = n * k`
#       ... = n * 1 : {`k = 1`}
#       ... = n     : mul_one),
# have trans : ∀n m r, m ∣ n → n ∣ r → m ∣ r, from
#   take n m r,
#   assume `m ∣ n` `n ∣ r`,
#   obtain k `n = m * k`, from `m ∣ n`,
#   obtain l `r = n * l`, from `n ∣ r`,
#   have r = m * (k * l), from calc
#      r = n * l       : `r = n * l`
#    ... = (m * k) * l : {`n = m * k`}
#    ... = m * (k * l) : mul.assoc,
#   exists.intro (k * l) `r = m * (k * l)`,
# and.intro refl (and.intro antisymm trans)
# #+END_SRC

As an exercise, try to prove the following properties of divisibility in Lean.

#+BEGIN_SRC lean
import data.nat
open nat

example : ∀ n m : ℕ, m ∣ m * n := 
sorry

example : ∀ n m k : ℕ, m ∣ n → m ∣ n * k := 
sorry

example : ∀ n m k : ℕ, m ∣ n → k * m ∣ k * n :=
sorry
#+END_SRC

# -- PROOFS --
# example : ∀n m, m ∣ m * n :=
# take n m,
# exists.intro n rfl
# 
# example : ∀n m k, m ∣ n → m ∣ n * k :=
# take n m k,
# assume `m ∣ n`,
# obtain l `n = m * l`, from `m ∣ n`,
# have n * k = m * (l * k), from calc
#   n * k = (m * l) * k : {`n = m * l`}
#     ... = m * (l * k) : mul.assoc,
# exists.intro (l * k) `n * k = m * (l * k)`
# 
# example : ∀n m k, m ∣ n → k * m ∣ k * n :=
# take n m k,
# assume `m ∣ n`,
# obtain l `n = m * l`, from `m ∣ n`,
# have k * n = (k * m) * l, from calc
#   k * n = k * (m * l) : {`n = m * l`}
#     ... = (k * m) * l : mul.assoc,
# exists.intro l `k * n = (k * m) * l`

We can also define divisibility on the integers in the same way. If $a$ and $b$ are integers, then
$a \mid b$ means that there is an integer $c$ such that $a \cdot c = b$. Divisibility on the
integers has most of the same properties as divisibility on the natural numbers. One difference is
that it is not antisymmetric. This is because, for example, $5 \mid -5$ and $-5 \mid 5$, but 5 and
$-5$ are not equal. We will state some other useful facts about divisibility, but we omit the proofs.

-----
*Proposition*. For all integers $a$, $b$, $c$ we have
- if $a \mid b$ then $a \mid b\cdot c$
- if $a \mid b$ and $a \mid c$, then $a \mid b + c$ and $a \mid b - c$.
-----

** Modular Arithmetic

In the discussion of equivalence relations in Chapter [[file:05_Equality.org::#Equality][Equality]] we
considered the example of the relation of modular equivalence on the
integers. This is sometimes thought of as "clock arithmetic."  Suppose
you have a 12-hour clock without a minute hand, so it only has an hour
hand which can point to the hours 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10,
11 and then it wraps to 12 again. We can do arithmetic with this
clock.
- If the hand currently points to 10, then 5 hours later it will point to 3.
- If the hand points to 7, then 23 hours before that, it pointed to 8. 
- If the hand points to 9, and we work for a 8 hours, then when we are
  done the hand will point to 5. If we worked twice as long, starting
  at 9, the hand will point to 1.

We want to write these statements using mathematical notation, so that
we can reason about them more easily. We cannot write $10 + 5 = 3$ for
the first expression, because that would be false, so instead we use
the notation $10 + 5 \equiv 3 \pmod{12}$. The notation $\pmod{12}$
indicates that we forget about multiples of 12, and we use the
"congruence" symbol with three horizontal lines to remind us that
these values are not exactly equal, but only equal up to multiples
of 12. The other two lines can be formulated as $7 - 23 \equiv 8
\pmod{12}$ and $9 + 2 \cdot 8 \equiv 1 \pmod{12}$.

Here are some more examples:
- $6 + 7 \equiv 1 \pmod{12}$
- $6 \cdot 7 \equiv 42 \equiv 6 \pmod{12}$
- $7 \cdot 5 \equiv 35 \equiv -1 \pmod{12}$
The last example shows that we can use negative numbers as well. 

We now give a precise definition.
-----
*Definition*. For integers $a$, $b$ and $n$ we say that $a$ and $b$ are /congruent modulo/ $n$ if $n
\mid a - b$. This is written $a \equiv b \pmod{n}$. The number $n$ is called the /modulus/.
-----
Typically we only use this definition when the modulus $n$ is positive.

-----
*Theorem*. Congruence modulo $n$ is an equivalence relation.

*Proof*. We have to show that congruence modulo $n$ is reflexive, symmetric and transitive.

It is reflexive, because $a - a = 0$, so $n \mid a - a$, and hence $a\equiv a \pmod{n}$.

To show that it is symmetric, suppose that $a \equiv b \pmod{n}$. Then by definition, $n \mid a -
b$. So $n \mid (-1) \cdot (a - b)$, which means that $n \mid b - a$. This means by definition that
$b \equiv a \pmod{n}$.

To show that it is transitive, suppose that $a \equiv b \pmod{n}$ and $b \equiv c \pmod{n}$. Then we
have $n \mid a - b$ and $n \mid b - c$. Hence by the previous proposition we have $n \mid (a - b) +
(b - c)$ which means that $n \mid a - c$. So $a \equiv c \pmod{n}$.
-----

This theorem justifies the "chaining" notation we used above when we
wrote $7 \cdot 5 \equiv 35 \equiv -1 \pmod{12}$. Since congruence
modulo 12 is transitive, we can now actually conclude that $7\cdot
5\equiv -1 \pmod{12}$.

-----
*Theorem*. Suppose that $a\equiv b \pmod{n}$ and $c\equiv d\pmod{n}$. Then $a+c\equiv b+d \pmod{n}$
and $a\cdot c\equiv b\cdot d\pmod{n}$.

Moreover, if $a\equiv b \pmod{n}$ then $a^k\equiv b^k \pmod{n}$ for all natural numbers $k$.

*Proof*. We know that $n \mid a - b$ and $n \mid c - d$. For the first statement, we can calculate
that $(a + c) - (b + d) = (a - b) + (c - d)$, so we can conclude that $n \mid (a + c) - (b + d)$
hence that $a+c\equiv b+d\pmod{n}$.

For the second statement, we want to show that $n \mid a\cdot c - b\cdot d$. We can factor $a\cdot
c - b\cdot d = (a - b)\cdot c + b\cdot(c-d)$. Now $n$ divides both summands on the right, hence $n$ divides $a\cdot
c - b\cdot d$, which means that $a\cdot c\equiv b\cdot d\pmod{n}$.

The last statement follows by repeatedly applying the second statement:
\begin{equation*}
a^k = \underbrace{a\cdot a \cdot \cdots \cdot a}_{\text{$k$ times}} \equiv \underbrace{b\cdot b \cdot \cdots \cdot b}_{\text{$k$ times}} = b^k \pmod{n}
\end{equation*}
-----

This theorem is useful for carrying out computations modulo $n$. Here are some examples.
- Suppose we want to compute $77 \cdot 123$ modulo 12. We know that $77 \equiv 5 \pmod{12}$ and $123
  \equiv 3 \pmod{12}$, so $77 \cdot 123 \equiv 5 \cdot 3 \equiv 15 \equiv 3 \pmod{12}$
- Suppose we want to compute $99 \cdot 998$ modulo 10. We know that $99 \equiv
  -1\pmod{10}$ and $998 \equiv -2 \pmod{10}$, hence $99 \cdot 998 \equiv (-1) \cdot (-2) \equiv 2 \pmod{10}$.
- Suppose we want to know the last digit of $101^{101}$. Notice that the last digit of a number $n$
  is congruent to $n$ modulo 10, so we can just compute $101^{101} \equiv 1^{101} \equiv 1
  \pmod{10}$. So the last digit of $101^{101}$ is 1.
- You are not allowed to compute in exponents with modular arithmetic. For example $8 \equiv 3 \pmod{5}$,
  but $2^8 \not\equiv 2^3 \pmod{5}$. To see this: $2^8 = 256 \equiv 1 \pmod{5}$, but $2^3 = 8 \equiv
  3 \pmod{5}$.

Some exercises (you are not allowed to use a calculator):

- Find the last digit of $99^{99}$. Can you also find the last two digits of this number?
- Prove that $50^{22} - 22^{50}$ is divisible by 7.
- Prove using modular arithmetic that for any two integers $m$ and $n$ that $m + n$ and $m - n$ have
  the same parity. Notice that $m$ and $n$ have the same parity precisely when they are congruent
  modulo 2.

Recall the quotient-remainder theorem from Chapter [[file:04_First_Order_Logic.org::#First_Order_Logic][First Order Logic]]:
if $n > 0$, then any integer $a$ can be expressed as $a = n q + r$,
where $0 \le r < n$. In the language of modular arithmetic this means
that $a \equiv r \pmod{n}$. So if $n > 0$, then every integer is
congruent to a number between 0 and $n-1$ (inclusive). So there "are
only $n$ different numbers" when working modulo $n$. This can be used
to prove many statements about the natural numbers.

-----
*Proposition*. For every integer $k$, $k^2+1$ is not divisible by 3.

*Proof*. Translating this problem to modular arithmetic, we have to show that $k^2+1 \not\equiv 0
\pmod{3}$ or in other words that $k^2\not\equiv 2 \pmod{3}$ for all $k$. By the quotient-remainder
theorem, we know that $k$ is either congruent to 0, 1 or 2, modulo 3. In the first case, $k^2\equiv
0^2\equiv 0\pmod{3}$. In the second case, $k^{2}\equiv 1^2 \equiv 1 \pmod{3}$, and in the last case
we have $k^{2}\equiv2^2\equiv4\equiv1\pmod{3}$. In all of those cases, $k^2\not\equiv2\pmod{3}$. So
$k^2+1$ is never divisible by 3.
-----
*Proposition*. For all integers $a$ and $b$, $a^2+b^2-3$ is not divisible by 4.

*Proof*. We first compute the squares modulo 4. We compute
\begin{align*}
0^2&\equiv 0\pmod{4}\\
1^2&\equiv 1\pmod{4}\\
2^2&\equiv 0\pmod{4}\\
3^2&\equiv 1\pmod{4}
\end{align*}
Since every number is congruent to 0, 1, 2 or 3 modulo 4, we know that every square is congruent to
0 or 1 modulo 4. This means that there are only four possibilities for $a^2+b^2\pmod{4}$. It can be
congruent to $0+0$, $1+0$, $0+1$ or $0+0$. In all those cases, $a^2+b^2\not\equiv 3\pmod{4}$ Hence
$4\nmid a^2+b^2-3$, proving the proposition.
-----

Exercises:
- Show that for every integer $n$ the number $n^4$ is congruent to 0 or 1 modulo 5. Hint: to
  simplify the computation, use that $4^4\equiv(-1)^4\pmod{5}$.
- Prove that the equation $n^4+m^4=k^4+3$ cannot hold for integers $n, m, k$. Hint: what are the
  possible values for the left hand side modulo 5? And for the right hand side?


# Do this later!
# ** Geometry
