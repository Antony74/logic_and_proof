#+Title: Logic and Proof
#+Author: [[http://www.andrew.cmu.edu/user/avigad][Jeremy Avigad]], [[http://www.andrew.cmu.edu/user/rlewis1/][Robert Y. Lewis]],  [[http://www.contrib.andrew.cmu.edu/~fpv/][Floris van Doorn]]

* Functions
:PROPERTIES:
  :CUSTOM_ID: Functions
:END:

In the late nineteenth century, developments in a number of branches
of mathematics pushed towards a uniform treatment of sets, functions,
and relations. We have already considered sets and relations. In this
chapter, we consider functions and their properties.

A function, $f$, is ordinary understood as a mapping from a domain
$X$ to another domain $Y$. In set-theoretic foundations, $X$ and $Y$
are arbitrary sets. We have seen that in a type-based system like
Lean, it is natural to distinguish between types and subsets of a
type. In other words, we can consider a type =X= of elements, and a
set =A= of elements of that type. Thus, in the type-theoretic
formulation, it is natural to consider functions between types =X= and
=Y=, and consider their behavior with respect to subsets of =X= and
=Y=.

In everyday mathematics, however, set-theoretic language is common,
and most mathematicians think of a function as a map between
sets. When discussing functions from a mathematical standpoint,
therefore, we will also adopt this language, and later switch to the
type-theoretic representation when we talk about formalization in
Lean.

** The Function Concept

If $X$ and $Y$ are any sets, we write $f : X \to Y$ to express the
fact that $f$ is a function from $X$ to $Y$. This means that $f$
assigns a value $f(x)$ in $X$ to every element $x$ of $X$. The set $X$
is called the /domain/ of $f$, and the set $Y$ is called the
/codomain/. (Some authors use the word "range" for the codomain, but
today it is more common to use the word "range" for what we call the
/image/ of $A$ below. We will avoid the ambiguity by avoiding the word
range altogether.)

The simplest way to define a function is to give its value at every
$x$ with an explicit expression. For example, we can write any of the
following:
- Let $f : \NN \to \NN$ be the function defined by $f(n) = n + 1$.
- Let $g : \RR \to \RR$ be the function defined by $g(x) = x^2$.
- Let $h : \NN \to \NN$ be the function defined by $h(n) = n^2$.
- Let $k : \NN \to \{0, 1\}$ be the function defined by
\[
k(n) =
  \left\{\begin{array}{ll}
    0 & \mbox{if $n$ is even} \\
    1 & \mbox{if $n$ is odd}
  \end{array}\right.
\]
The ability to define functions using an explicit expression raises
the foundational question as to what counts as legitimate
"expression." For the moment, let us set that question aside, and
simply note that modern mathematics is comfortable with all kinds of
exotic definitions. For example, we can define a function $f : \RR \to
\{0, 1\}$ by
\[
f(x) =
  \left\{\begin{array}{ll}
    0 & \mbox{if $x$ is rational} \\
    1 & \mbox{if $x$ is irrational}
  \end{array}\right.
\]
This is at odds with a view of functions as objects that are
computable in some sense. It is not at all clear what it means to be
presented with a real number as input, let alone whether it is
possible to determine, algorithmically, whether such a number is
rational or not. We will return to discuss such issues in a later
chapter.

Notice that the choice of the variables $x$ and $n$ in the definitions
above are arbitrary. They are bound variables in that the functions
being defined do not depend on $x$ or $n$. The values remain the same
under renaming, just as the truth values of "for every $x$, $P(x)$"
and "for every $y$, $P(y)$" are the same. Given an expression $e(x)$
that depends on the variable $x$, logicians often use the notation
$\lam x e(x)$ to denote the function that maps $x$ to $e(x)$. This is
called "lambda notation," for the obvious reason, and it is often
quite handy. Instead of saying "let $f$ be the function defined by
$f(x) = x+1$," we can say "let $f = \lam x (x + 1)$." This is /not/
common mathematical notation, and it is best to avoid it
unless you are talking to logicians or computer scientists. We will
see, however, that lambda notation is built in to Lean.

For any set $X$, we can define a function $i_X(x)$ by the equation
$i_X(x) = x$. This function is called the /identity function/. More
interestingly, let $f : X \to Y$ and $g : Y \to Z$. We can define a
new function $k : X \to Z$ by $k(x) = g(f(x))$. The function $k$ is
called /the composition of $f$ and $g$/ or /$f$ composed with $g$/ and
it is written $g \circ f$. The order is somewhat confusing; you just
have to keep in mind that to evaluate the expression $g(f(x))$ you
first evaluate $f$ on input $x$, and then evaluate $g$.

We think of two functions $f, g : X \to Y$ as being equal, or the same
function, when for they have the same values on every input; in other
words, for every $x$ in $X$, $f(x) = g(x)$. For example, if
$f, g : \RR \to \RR$ are defined by $f(x) = x + 1$ and $g(x) = 1 + x$,
then $f = g$. Notice that the statement that two functions are equal
is a universal statement (that is, for the form "for every $x$, ...").

-----

*Proposition.* For every $f : X \to Y$, $f \circ i_X = f$ and $i_Y
\circ f = f$.

*Proof.* Let $x$ be any element of $X$. Then $(f \circ i_X)(x) =
f(i_X(x)) = f(x)$, and $(i_Y \circ f)(x) = i_Y(f(x)) = x$.

-----

Suppose $f : X \to B$ and $g : Y \to A$ satisfy $g \circ f =
i_A$. Remember that this means that $g(f(x)) = x$ for every $x$ in
$A$. In that case, $g$ is said to be a /left inverse/ to $f$, and $f$
is said to be a /right inverse/ to $g$. Here are some examples:
- Define $f, g : \RR \to \RR$ by $f(x) = x + 1$ and $g(x) = x -
  1$. Then $g$ is both a left and a right inverse to $f$, and vice-versa.
- Write $\RR^{\geq 0}$ to denote the nonnegative reals. Define
  $f : \RR \to \RR^{\geq 0}$ by $f(x) = x^2$, and define $g :
  \RR^{\geq 0} \to \RR$ by $g(x) = \sqrt x$. Then $f(g(x)) = (\sqrt
  x)^2 = x$ for every $x$ in the domain of $g$, so $f$ is a left
  inverse to $g$, and $g$ is a right inverse to $f$. On the other
  hand, $g(f(x)) = \sqrt{x^2} = | x |$, which is not the same as $x$
  when $x$ is negative. So $g$ is not a left inverse to $f$, and $f$
  is not a right inverse to $g$.

The following fact is not at all obvious, even though the proof is short:

-----

*Proposition.* Suppose $f : X \to Y$ has a left inverse,
$h$, and a right inverse $k$. Then $h = k$.

*Proof.* Let $y$ be any element in $B$. The idea is to compute
$h(f(k(y))$ in two different ways. Since $h$ is a left inverse to $f$,
we ahve $h(f(k(y))) = k(y)$. On the other hand, since $k$ is a right
inverse to $f$, $f(k(y)) = y$, and so $h(f(k(y)) = h(y)$. So $k(y) =
h(y)$.

-----


If $g$ is both a right and left inverse to $f$, we say that $g$ is
simply the inverse of $f$. X function $f$ may have more than one left
or right inverse (we leave it to you to cook up examples), but it can
have at most one inverse.

-----

*Proposition.* Suppose $g_1, g_2 : Y \to X$ are both inverse to
$f$. Then $g_1 = g_2$.

*Proof.* The follows from the previous proposition, since (say) $g_1$
is a left inverse to $f$, and $g_2$ is a right inverse.

-----

When $f$ has an inverse, $g$, this justifies calling $g$ /the/ inverse
to $f$, and writing $f^{-1}$ to denote $g$. Notice that if $f^{-1}$ is
an inverse to $f$, then $f$ is an inverse to $f^{-1}$. So if $f$ has
an inverse, then so does $f^{-1}$, and $(f^{-1})^{-1} = f$. For any
set $A$, clearly we have $i_X^{-1} = i_X$.

-----

*Proposition.* Suppose $f : X \to Y$ and $g : Y \to Z$. If $h : Y \to X$ is a
left inverse to $f$ and $k : Z \to Y$ is a left inverse to $g$, then
$h \circ k$ is a left inverse to $g \circ f$.

*Proof.* For every $x$ in $X$,
\[
(h \circ k) \circ (g \circ f) (x) = h(k(g(f(x)) = h(f(x)) = x.
\]

*Corollary.* The previous proposition holds with "left" replaced by
"right".

*Proof.* Switch the role of $f$ with $h$ and $g$ with $k$ in the
previous proposition.

*Corollary.* If $f : X \to Y$ and $g : Y \to Z$ both have inverses,
then $(f \circ g)^{-1} = g^{-1} \circ f^{-1}$.

-----

# some pictures here would be helpful.

** Injective, Surjective, and Bijective Functions

A function $f : X \to Y$ is said to be /injective/, or an /injection/,
or /one-one/, if given any $x_1$ and $x_2$ in $A$, if $f(x_1) = f(x_2)$, then
$x_1 = x_2$. Notice that the conclusion is equivalent to its
contrapositive: if $x_1 \neq x_2$, then $f(x_1) \neq f(x_2)$. So $f$ is
injective if it maps distinct element of $X$ to distinct elements of
$Y$.

A function $f : X \to Y$ is said to be /surjective/, or a
/surjection/, or /onto/, if for every element $y$ of $Y$, there is an
$x$ in $X$ such that $f(x) = y$. In other words, $f$ is surjective if
every element in the codomain is the value of $f$ at some element in
the domain.

A function $f : X \to Y$ is said to be /bijective/, or a /bijection/,
or a /one-to-one correspondence/, if it is both injective and
surjective. Intuitively, if there is a bijection between $X$ and $Y$,
then $X$ and $Y$ have the same size, since $f$ makes each element of
$X$ correspond to exactly one element of $Y$ and vice-versa. For
example, it makes sense to interpret the statement that there were four
Beatles as the statement that there is a bijection between the set
$\{1, 2, 3, 4\}$ and the set $\{ \text{John, Paul, George, Ringo} \}$.
If we claimed that there were /five/ Beatles, as evidenced by the
function $f$ which assigns 1 to John, 2 to Paul, 3 to George, 4 to
Ringo, and 5 to John, you should object that we double-counted John
--- that is, $f$ is not injective. If we claimed there were only three
Beatles, as evidenced by the function $f$ which assigns 1 to John, 2
to Paul, and 3 to George, you should object that we left out poor
Ringo --- that is, $f$ is not surjective.

The next two propositions show that these notions can be cast in
terms of the existence of inverses.

-----

*Proposition.* Let $f : X \to Y$.
- If $f$ has a left inverse, then $f$ is injective.
- If $f$ has a right inverse, then $f$ is surjective.
- If $f$ has an inverse, then it is $f$ bijective.

*Proof.* For the first claim, suppose $f$ has a left inverse $g$, and
suppose $f(x_1) = f(x_2)$. Then $g(f(x_1)) = g(f(x_2))$, and so $x_1 =
x_2$.

For the second claim, suppose $f$ has a right inverse $h$. Let $y$ be
any element of $Y$, and let $x = g(y)$. Then $f(x) = f(g(y)) = y$.

The third claim follows from the first two.

-----

The following proposition is more interesting, because it requires us
to define new functions, given hypotheses on $f$.

-----

*Proposition.* Let $f : X \to Y$.
- If $X$ is nonempty and $f$ is injective, then $f$ has a left
  inverse.
- If $f$ is surjective, then $f$ has a right inverse.
- If $f$ if bijective, then it has an inverse.

*Proof.* For the first claim, let $\hat x$ be any element of $X$, and
suppose $f$ is injective. Define $g : Y \to X$ by setting $g(y)$ equal
to any $x$ such that $f(x) = y$, if there is one, and $\hat x$
otherwise. Now, suppose $g(f(x)) = x'$. By the definition of $g$, $x'$
has to have the property that $f(x) = f(x')$. Since $f$ is injective,
$x = x'$, so $g(f(x)) = x$.

For the second claim, because $f$ is surjective, we know that for
every $y$ in $Y$ there is any $x$ such that $f(x) = y$. Define $h : B
\to A$ by again setting $h(y)$ equal to any such $x$. (In contrast to
the previous paragraph, here we know that such an $x$ exists, but it
might not be unique.) Then, by the definition of $h$, we have $f(h(y))
= y$.

-----

Notice that the definition of $g$ in the first part in the first part
of the proof requires the function to "decide" whether there is an $x$
in $X$ such that $f(x) = y$. There is nothing mathematically dubious
about this definition, but if many situations, this cannot be done
/algorithmically/; in other words, $g$ might not be computable from
the data. More interestingly, the definition of $h$ in the second part
of the proof requires the function to "choose" a suitable value of $x$
from among potentially many candidates. We will see later that this is
a version of the /axiom of choice/. In the early twentieth century,
the use of the axiom of choice in mathematics was hotly debated, but
today it is commonplace.

Using these equivalances and the results in the previous section, we
can prove the following:

-----

*Proposition.* Let $f : X \to B$ and $g : Y \to Z$.
- if $f$ and $g$ are injective, then so is $g \circ f$.
- if $f$ and $g$ are surjective, then so is $g \circ f$.

*Proof.* If $f$ and $g$ are injective, then they have left inverses
$h$ and $k$, respectively, in which case $h \circ k$ is a left inverse
to $g \circ f$. The second statement is proved similarly.

-----

Once can prove these two statements, however, without mentioning
inverses at all. We leave that to you as an exercise.

Notice that the expression $f(n) = 2 n$ can be used to define
infinitely many functions with domain $\NN$, such as:
- a function $f : \NN \to \NN$
- a function $f : \NN \to \RR$
- a function $f: \NN \to \{ n \; | \; n \text{ is even} \}$
Only the third one is surjective. Thus a specification of the
function's codomain as well as the domain is essential to making sense
of whether a function is surjective.

** Functions and Subsets of the Domain

Suppose $f$ is a function from $X$ to $Y$. We may wish to reason about
the behavior of $f$ on some subset $A$ of $X$. For example, we can say
that $f$ /is injective on/ $A$ if for every $x_1$ and $x_2$ in $A$, if
$f(x_1) = f(x_2)$, then $x_1 = x_2$.

If $f$ is a function from $X$ to $Y$ and $A$ is a subset of $X$, we
write $f[A]$ to denote the /image of/ $f$ /on/ $A$, defined by
\[
f[A] = \{ y \in Y \; | \; y = f(x) \mbox{for some $x$ in $A$} \}.
\]
In words, $f[A]$ is the set of elements of $Y$ that are "hit" by
elements of $A$ under the mapping $f$. Notice that there is an
implicit existential quantifier here, so that reasoning about images
invariables involves the corresponding rules. 

-----

*Proposition.* Suppose $f : X \to Y$, and $A$ is a subset of
$X$. Then for any $x$ in $A$, $f(x)$ is in $f[A]$.

*Proof.* By definition, $f(x)$ is in $f[A]$ if and only if there
is some $x'$ in $A$ such that $f(x') = f(x)$. But that holds for $x' = x$.

*Proposition.* Suppose $f : X \to Y$ and $g : Y \to Z$. Let $A$ be
a subset of $X$. Then
\[
(g \circ f)[A] = g[f[A]].
\]

*Proof.* Suppose $z$ is in $(g \circ f)[A]$. Then for some $x \in
A$, $z = (g \circ f)(x) = g(f(x))$. By the previous proposition,
$f(x)$ is in $f[A]$. Again by the previous proposition, $g(f(x))$ is
in $g[f[A]]$.

Conversely, suppose $z$ is in $g[f[A]]$. Then there is a $y$ in $f[A]$
such that $f(y) = z$, and since $y$ is in $f[D]$, there is an $x$ in
$A$ such that $f(x) = y$. But then $(g \circ f)(x) = g(f(x)) = g(y) =
z$, so $z$ is in $(g \circ f)[A]$.

-----

Notice that if $f$ is a function from $X$ to $Y$, then $f$ is
surjective if and only if $f[X] = Y$. So the previous proposition is a
generalization of the fact that the composition of surjective
functions is surjective. 

Suppose $f$ is a function from $X$ to $Y$, and $A$ is a subset of
$X$. We can /view/ $f$ as a function from $A$ to $Y$, by simply
ignoring the behavior of $f$ on elements outside of $A$. Properly
speaking, this is another function, denoted $f \upharpoonright$ and
called "the restriction of $f$ to $A$." In other words, given $f : X
\to Y$ and $A \subseteq X$, $f \upharpoonright A : A \to Y$ is the
function defined by $(f \upharpoonright A)(x) = x$ for every $x$ in
$A$. Notice that now "$f$ is injective on $A$" means simply that the
restriction of $f$ to $A$ is injective.

There is another important operation on functions, known as the
/preimage/. If $f : X \to Y$ and $B \subseteq Y$, then the
/preimage of $B$ under $f$/, denoted $f^{-1}[B]$, is defined by
\[
f^{-1}[B] = \{ x \in X \; | \; f(x) \in B \},
\]
that is, the set of elements of $X$ that get mapped into $B$. Notice
that this makes sense even if $f$ does not have an inverse; for a
given $y$ in $B$, there may be no $x$'s with the propery $f(x) \in B$,
or there may be many. If $f$ has an inverse, $f^{-1}$, then for every
$y$ in $B$ there is exactly one $x \in X$ with the property $f(x) \in
B$, in which case, $f^{-1}[B]$ means the same thing whether you
interpret it as the image of $B$ under $f^{-1}$ or the preimage of $B$
under $f$.

-----

*Proposition.* Suppose $f : X \to Y$ and $g : Y \to Z$. Let $C$ be
a subset of $Z$. Then
\[
(g \circ f)^{-1}[C] = g^{-1}[f^{-1}[A]].
\]

-----

# somehow credit MathOverflow for the next list:
# http://math.stackexchange.com/questions/359693/overview-of-basic-results-about-images-and-preimages

Here we give a long list of facts properties of images and
preimages. Here, $f$ denotes an arbitrary function from $X$ to $Y$,
$A, A_1, A_2, \ldots$ denote arbitrary subsets of $X$, and $B, B_1,
B_2, \ldots$ denote arbitrary subsets of $Y$.

- $A \subseteq f^{-1}[f[A]]$, and if $f$ is injective, $A =
  f^{-1}[f[A]]$.
- $f[f^{-1}[B]] \subseteq B$, and if $f$ is surjective, $B =
  f[f^{-1}[B]]$.
- If $A_1 \subseteq A_2$, then $f[A_1] \subseteq f[A_2]$.
- If $B_1 \subseteq B_2$, then $f^{-1}[B_1] \subseteq f^{-1}[B_2]$.
- $f[A_1 \cup A_2] = f[A_1] \cup f[A_2]$.
- $f^{-1}[B_1 \cup B_2] = f^{-1}[B_1] \cup f^{-1}[B_2]$.
- $f[A_1 \cap A_2] \subseteq f[A_1 \cap A_2]$, and if $f$ is
  injective, $f[A_1 \cap A_2] = f[A_1 \cap A_2]$.
- $f^{-1}[B_1 \cap B_2] = f^{-1}[B_1] \cap f^{-1}[B_2]$.
- $f[A] \setminus f[B] \subseteq f[A \setminus B]$.
- $f^{-1}[A] \setminus f^{-1}[B] \subseteq f[A \setminus B]$.
- $f[A] \cap B = f[A \cap f^{-1}[B]]$.
- $f[A] \cup B \supseteq f[A \cup f^{-1}[B]]$.
- $A \cap f^{-1}[B] \subseteq f^{-1}[f[A] \cap B]$.
- $A \cup f^{-1}[B] \subseteq f^{-1}[f[A] \cup B]$.

# add facts for arbitrary unions and intersections.

** Functions and Relations
:PROPERTIES:
  :CUSTOM_ID: Functions_and_Relations
:END:      

A binary relation $R(x,y)$ on $A$ and $B$ is /functional/ if for every
$x$ in $A$ there exists a unique $y$ in $B$ such that $R(x,y)$. If $R$
is a functional relation, we can define a function $f_R : X \to B$ by
setting $f_R(x)$ to be equal to the unique $y$ in $B$ such that
$R(x,y)$. Conversely, it is not hard to see that if $f : X \to B$ is
any function, the relation $R_f(x, y)$ defined by $f(x) = y$ is a
functional relation. The relation $R_f(x,y)$ is known as the /graph/
of $f$.

It is not hard to check that functions and relations travel in pairs:
if $f$ is the function associated with a functional relation $R$, then
$R$ is the functional relation associated the function $f$, and
vice-versa. In set-theoretic foundations, a function is
often defined /to be/ a functional relation. Conversely, we have seen
that in type-theoretic foundations like the one adopted by Lean,
relations are often defined to be certain types of functions. We will
discuss these matters later on, and in the meanwhile only remark that
in everyday mathematical practice, the foundational details are not so
important; what is important is simply that every function has a
graph, and that any functional relation can be used to define a
corresponding function.

So far, we have been focusing on functions that take a single
argument. We can also consider functions $f(x, y)$ or $g(x, y, z)$
that take multiple arguments. For example, the addition function $f(x,
y) = x + y$ on the integers takes two integers and returns an
integer. Remember, we can consider binary functions, ternary
functions, and so on, and the number of arguments to a function is
called its "arity."  One easy way to make sense of functions with
multiple arguments is to think of them as unary functions on sets of
tuples. For example, if $A$ and $B$ are sets, $A \times B$ consists of
the set of ordered pairs $(a, b)$ where $a$ in an element of $A$ and
$b$ is an element of $B$. We can think of a function $f$ which takes
two arguments, one in $A$ and one in $B$, and returns an argument in
$C$ as a unary function from $A \times B$ to $C$, whereby $f(a,
b)$ abbreviates $f((a, b))$. We have seen that in dependent type
theory (and in Lean) it is more convenient to think of such a function
$f$ as a function which takes an element of $A$ and returns a function
from $B \to C$, so that $f(a, b)$ abbreviates $(f(a))(b)$. Such a
function $f$ maps $A$ to $B \to C$, where $B \to C$ is the set of
functions from $B$ to $C$.

We will return to these different ways of modeling functions of higher
arity later on, when we consider set-theoretic and type-theoretic
foundations. One again, we remark that in ordinary mathematics, the
foundational details to not matter much. The two choices above are
inter-translatable, and sanction the same principles for reasoning
about functions informally.

In mathematics, we often also consider the notion of a /partial
function/ from $X$ to $Y$, which is really a function from some subset
of $X$ to $Y$. The fact that $f$ is a partial function from $X$ to $Y$
is sometimes written $f : X \nrightarrow Y$, which should be
interpreted as saying that $f : A \to Y$ for some subset $A$ of
$Y$. Intuitively, we think of $f$ as a function from $X \to Y$ which
is simply "undefined" at some of its inputs; for example, we can think
of $f : \mathbb{R} \nrightarrow \mathbb{R}$ defined by $f(x) = 1 / x$,
which is undefined at $x = 0$, so that in reality $f : \mathbb{R}
\setminus \{ 0 \} \to R$. The set $A$ is sometimes called the
/domain of $f$/, in which case, there is no good name for $X$;
others continue to call $X$ the domain, and refer to $A$ as the
/domain of definition/. To indicate that a function $f$ is
defined at $x$, that is, that $x$ is in the domain of definition of
$f$, we sometimes write $f(x) \downarrow$. If $f$ and $g$ are two
partial functions from $X$ to $Y$, we write $f(x) \simeq g(x)$ to mean
that either $f$ and $g$ are both defined at $x$ and have the same
value, or are both undefined at $x$. Notions of injectivity,
surjectivity, and composition are extended to partial functions,
generally as you would expect them to be.

In terms of relations, a partial function $f$ corresponds to a
relation $R_f(x,y)$ such that for every $x$ there is at most one $y$
such that $R_f(x,y)$ holds. Mathematicians also sometimes consider
/multifunctions/ from $X$ to $Y$, which correspond to relations
$R_f(x,y)$ such that for every $x$ in $X$, there is /at least/
one $y$ such that $R_f(x,y)$ holds. There may be many such $y$; you
can think of these as functions which have more than one input
value. If you think about it for a moment, you will see that a
/partial multifunction/ is essentially nothing more than an arbitrary
relation.

** Functions and Symbolic Logic

Even though we have avoided the use of quantifiers and logical symbols
in the definitions above, by now you should be seeing them lurking
beneath the surface. That fact that two functions $f, g : X \to Y$ are
equal if and only if they take the same values at every input can be
expressed as follows:
\[
\fa {x \in X} (f(x) = g(x)) \liff f = g
\]
This principle is a known as /function extensionality/. Recall that
the notation $\fa {x \in X} P(x)$ abbreviates $\fa x (x \in X \to
P(x))$, and $\ex {x \in X} P(x)$ abbreviates $\ex x (x \in X \wedge
P(x))$, thereby relativizing the quantifiers to $A$.

We can avoid set-theoretic notation if we assume we are working in a
logical formalism with basic types for $X$ and $Y$, so that we can
specify that $x$ ranges over $X$. In that case, we will write instead
\[ \fa {x : X} (f(x) = g(x)) \liff f = g \] to indicate that the
quantification is over $X$. Henceforth, we will assume that all our
variables range over some type, though we will sometimes omit the
types in the quantifiers when they can be inferred from context.

The function $f$ is injective if it satisfies
\[
\fa {x_1, x_2 : X} (f(x_1) = f(x_2) \to x_1 = x_2),
\]
and $f$ is surjective if
\[
\fa {y : Y} \ex {x : X} (f(x) = y).
\]
If $f : X \to Y$ and $g: Y \to X$, $g$ is a left inverse
to $f$ if
\[
\fa {x : X} g(f(x)) = a.
\]
Notice that this is a universal statement, and it is equivalent to the
statement that $f$ is a right inverse to $g$.

Remember that in logic it is common to use lambda notation to define
functions. We can denote the identity function by $\lam x x$, or
perhaps $\lam {x : X} x$ to emphasize that the domain of the function
is $X$. If $f : X \to Y$ and $g : Y \to Z$, we can define the
composition $g \circ f$ by $g \circ f = \lam {x : X} g(f(x))$.

# later we should move this to the existential quantifier chapter.

Remember that if $P(x)$ is any predicate, then in first order logic we
can assert that there exists a unique $x$ satisfying $P(x)$, written
$\exunique x P(x)$, with the conjunction of the following two
statements:
- $\ex x P(x)$
- $\fa {x_1, x_2} (P(x_1) \wedge P(x_2) \to x_1 = y_1)$
Equivalently, we can write
\[
\ex x (P(x) \wedge \fa {x'} (P(x') \to x' = x)).
\]
Assuming $\exunique x P(x)$, the following two statements are
equivalent:
- $\ex x (P(x) \wedge Q(x))$
- $\fa x (P(x) \to Q(x))$
and both can be taken to assert that "the $x$ satisfying $P$ also
satisfies $Q$."

A binary relation $R$ on $X$ and $Y$ is functional if it satisfies
\[
\fa x \exunique y R(x,y).
\]
In that case, a logician might use "iota notation,"
\[
f(x) = \iota y \; R(x, y)
\]
to define $f(x)$ to be equal to the unique $y$ satsifying $R(x,y)$. If
$R$ satisfies the weaker property
\[
\fa x \ex y R(x,y),
\]
a logician might use "the Hilbert epsilon" to define a function
\[
f(x) = \varepsilon y \; R(x, y)
\]
to "choose" a value of $y$ satisfying $R(x, y)$. As we have noted
above, this is an implicit use of the axiom of choice.

** Second- and Higher-Order Logic

In contrast to first-order logic, where we start with a fixed stock of
function and relation symbols, the considerations in this chapter
encourage us to consider a more expressive language with variables
ranging over functions and relations as well. For example, saying that
a function $f : X \to Y$ has a left-inverse implicitly involves a
quantifying over functions, \[ \ex g \fa x g(f(x)) = x.  \] The
theorem that asserts that if any function $f$ from $X$ to $Y$ is
injective then it has a left-inverse can be expressed as follows: \[
\fa {x_1, x_2} (f(x_1) = f(x_2) \to x_1 = x_2) \to \ex g \fa x g(f(x))
= x.  \] Similarly, saying that two sets $X$ and $Y$ have a one-to-one
correspondence asserts the existence of a function $f : X \to Y$ as
well as an inverse to $f$. For another example, in Section [[#Functions_and_Relations][Functions
and Relations]] we asserted that every functional relation gives rise to
a corresponding function, and vice-versa.

What makes these statements interesting is that they involve
quantification, both existential and universal, over functions
and relations. This takes us outside the realm of first-order
logic. One option is to develop a theory in the language of
first-order logic in which the universe contains functions, and
relations as objects; we will see later that this is what axiomatic
set theory does. An alternative is to extend first-order logic to
involve new kinds of quantifiers and variables, to range over
functions and relations. This is what higher-order logic does.

There are various ways to go about this. In view of the relationship
between functions and relations described above, one can take
relations as basic, and define functions in terms of them, or
vice-versa. The following formulation of higher-order logic, due to
the logician Alonzo Church, follows the latter approach. It is
sometimes known as /simple type theory/.

Start with some basic types, $X, Y, Z, \ldots$ and a special type,
$\fn{Prop}$, of propositions. Add the following two rules to build new
types:
- If $U$ and $V$ are types, so is $U \times V$.
- If $U$ and $V$ are types, so is $U \to V$.
The first intended to denote the type of ordered pairs $(u, v)$, where
$u$ is in $U$ and $v$ is in $V$. The second is intended to denote the
type of functions from $U$ to $V$. Simple type theory now adds the
following means of forming expressions:
- If $u$ is of type $U$ and $v$ is of type $V$, $(u, v)$ is of type
  $v$.
- If $p$ is of type $U \times V$, then $(p)_1$ is of type $U$ and
  $(p)_2$ if of type $V$. (These are intended to denote the first and
  second element of the pair $p$.)
- If $x$ is a variable of type $U$, and $v$ is any expression of type
  $V$, then $\lam x v$ is of type $U \to V$.
- If $f$ is of type $U \to V$ and $u$ is of type $U$, $f(u)$ is of
  type $V$.
In addition, simple type theory provides all the means we have in
first-order logic --- boolean connectives, quantifiers, and equality
-- to build propositions. 

A function $f(x, y)$ which takes elements of $X$ and $Y$ to a type $Z$
is viewed as an object of type $X \times Y \to Z$. Similarly, a binary
relation $R(x,y)$ on $X$ and $Y$ is viewed as an object of type $X
\times Y \to \fn{Prop}$. What makes higher-order logic "higher order"
is that we can iterate the function type operation indefinitely. For
example, if $\NN$ is the type of natural numbers, $\NN \to \NN$
denotes the type of functions from the natural numbers to the natural
numbers, and $(\NN \to \NN) \to \NN$ denotes the type of functions
$F(f)$ which take a function as argument, and returns a natural number.

We have not specified the syntax and rules of higher-order logic very
carefully. This is done in a number of more advanced logic
textbooks. The fragment of higher-order logic which allows only
functions and relations on the basic types (without iterating these
constructions) is known as second-order logic.

These notions should seem familiar; we have been using these
constructions, with similar notation, in Lean. Indeed, Lean's logic is
an even more elaborate and expressive system of logic, which fully subsumes
all the notions of higher-order logic we have discussed here.

** Functions in Lean

The fact that the notions we have been discussing have such a
straightforward logical form means that it is easy to define them in
Lean. The main difference between the formal representation in Lean
and the informal representation above is that, in Lean, we distinguish
between a type =X= and a subset =A : set X= of that type.

In Lean's library, composition and identity are defined as follows:
#+BEGIN_SRC lean
namespace hide
-- BEGIN
variables {X Y Z : Type}

definition comp (f : Y → Z) (g : X → Y) : X → Z :=
λx, f (g x)

infixr  ` ∘ ` := comp

definition id (x : X) : X :=
x
-- END
end hide
#+END_SRC
Ordinarily, to use these definitions the notation, you use the command
=open function=. We omit this command here, because we are duplicating
the definitions, for expository purposes.

Ordinarily, we use =funext= (for "function extensionality") to prove
that two functions are equal.
#+BEGIN_SRC lean
variables {X Y : Type}

-- BEGIN
example (f g : X → Y) (H : ∀ x, f x = g x) : f = g := 
funext H
-- END
#+END_SRC
But Lean can prove some basic identities by simply unfolding
definitions and simplifying expressions, using reflexivity.

#+BEGIN_SRC lean
variables {X Y Z W : Type}

definition comp (f : Y → Z) (g : X → Y) : X → Z :=
λ x, f (g x)

infixr  ` ∘ ` := comp

-- BEGIN
lemma left_id (f : X → Y) : id ∘ f = f := rfl

lemma right_id (f : X → Y) : f ∘ id = f := rfl

theorem comp.assoc (f : Z → W) (g : Y → Z) (h : X → Y) : (f ∘ g) ∘ h = f ∘ (g ∘ h) := rfl

theorem comp.left_id (f : X → Y) : id ∘ f = f := rfl

theorem comp.right_id (f : X → Y) : f ∘ id = f := rfl
-- END
#+END_SRC

We can define what it means for $f$ to be injective, surjective, or
bijective:

#+BEGIN_SRC lean
variables {X Y Z : Type}

definition comp (f : Y → Z) (g : X → Y) : X → Z :=
λ x, f (g x)

infixr  ` ∘ ` := comp

-- BEGIN
definition injective (f : X → Y) : Prop := ∀ ⦃x₁ x₂⦄, f x₁ = f x₂ → x₁ = x₂

definition surjective (f : X → Y) : Prop := ∀ y, ∃ x, f x = y

definition bijective (f : X → Y) := injective f ∧ surjective f
-- END
#+END_SRC
Marking the variables =x₁= and =x₂= implicit in the definitionof
=injective= means that we do not have to write them as
often. Specifically, given =H : injective f=, and =H₁ x₁ : f x₁ = f
x₂=, we write =H H₁= rather than =H x₁ x₂ H₁= to show =x₁ = x₂=.

We can then prove that the identity function is bijective:
#+BEGIN_SRC lean
variables {X Y Z : Type}

definition comp (f : Y → Z) (g : X → Y) : X → Z :=
λ x, f (g x)

infixr  ` ∘ ` := comp


definition injective (f : X → Y) : Prop := ∀ ⦃x₁ x₂⦄, f x₁ = f x₂ → x₁ = x₂

definition surjective (f : X → Y) : Prop := ∀ y, ∃ x, f x = y

definition bijective (f : X → Y) := injective f ∧ surjective f

-- BEGIN
theorem injective_id : injective (@id X) := 
take x₁ x₂, 
assume H : id x₁ = id x₂, 
show x₁ = x₂, from H

theorem surjective_id : surjective (@id X) := 
take y, 
show ∃ x, id x = y, from exists.intro y rfl

theorem bijective_id : bijective (@id X) := and.intro injective_id surjective_id
-- END
#+END_SRC
More interestingly, we can prove that the composition of injective
functions is injective, and so on.
#+BEGIN_SRC lean
variables {X Y Z : Type}

definition comp (f : Y → Z) (g : X → Y) : X → Z :=
λ x, f (g x)

infixr  ` ∘ ` := comp

definition injective (f : X → Y) : Prop := ∀ ⦃x₁ x₂⦄, f x₁ = f x₂ → x₁ = x₂

definition surjective (f : X → Y) : Prop := ∀ y, ∃ x, f x = y

definition bijective (f : X → Y) := injective f ∧ surjective f

-- BEGIN
theorem injective_comp {g : Y → Z} {f : X → Y} (Hg : injective g) (Hf : injective f) :
  injective (g ∘ f) :=
take x₁ x₂, 
suppose (g ∘ f) x₁ = (g ∘ f) x₂, 
have f x₁ = f x₂, from Hg this,
show x₁ = x₂, from Hf this

theorem surjective_comp {g : Y → Z} {f : X → Y} (Hg : surjective g) (Hf : surjective f) :
  surjective (g ∘ f) :=
take z,
obtain y (Hy : g y = z), from Hg z,
obtain x (Hx : f x = y), from Hf y,
have g (f x) = z, from eq.subst (eq.symm Hx) Hy,
show ∃ x, g (f x) = z, from exists.intro x this

theorem bijective_comp {g : Y → Z} {f : X → Y} (Hg : bijective g) (Hf : bijective f) :
  bijective (g ∘ f) :=
obtain Hginj Hgsurj, from Hg,
obtain Hfinj Hfsurj, from Hf,
and.intro (injective_comp Hginj Hfinj) (surjective_comp Hgsurj Hfsurj)
-- END
#+END_SRC

The notions of left and right inverse are defined in the expected way.
#+BEGIN_SRC lean
variables {X Y : Type}

-- BEGIN
-- g is a left inverse to f
definition left_inverse (g : Y → X) (f : X → Y) : Prop := ∀ x, g (f x) = x

-- g is a right inverse to f
definition right_inverse (g : Y → X) (f : X → Y) : Prop := left_inverse f g
-- END
#+END_SRC
In particular, composing with a left or right inverse yields the
identity.
#+BEGIN_SRC lean
variables {X Y Z : Type}

definition comp (f : Y → Z) (g : X → Y) : X → Z :=
λ x, f (g x)

infixr  ` ∘ ` := comp

definition left_inverse (g : Y → X) (f : X → Y) : Prop := ∀ x, g (f x) = x

definition right_inverse (g : Y → X) (f : X → Y) : Prop := left_inverse f g

-- BEGIN
definition id_of_left_inverse {g : Y → X} {f : X → Y} : left_inverse g f → g ∘ f = id :=
assume H, funext H

definition id_of_right_inverse {g : Y → X} {f : X → Y} : right_inverse g f → f ∘ g = id :=
assume H, funext H
-- END
#+END_SRC
Notice that we need to use =funext= to show the equality of functions.

The following hsows that if a function has a left inverse, then it is
injective, and it it has a right inverse, then it is surjective.
#+BEGIN_SRC lean
variables {X Y : Type}

definition injective (f : X → Y) : Prop := ∀ ⦃x₁ x₂⦄, f x₁ = f x₂ → x₁ = x₂

definition surjective (f : X → Y) : Prop := ∀ y, ∃ x, f x = y

definition left_inverse (g : Y → X) (f : X → Y) : Prop := ∀x, g (f x) = x

definition right_inverse (g : Y → X) (f : X → Y) : Prop := left_inverse f g

-- BEGIN
theorem injective_of_left_inverse {g : Y → X} {f : X → Y} : left_inverse g f → injective f :=
assume h, take x₁ x₂, assume feq,
calc x₁ = g (f x₁) : by rewrite h
    ... = g (f x₂) : feq
    ... = x₂       : by rewrite h

theorem surjective_of_right_inverse {g : Y  → X} {f : X → Y} : right_inverse g f → surjective f :=
assume h, take y,
let  x : X := g y in
have f x = y, from calc
  f x  = (f (g y))    : rfl
   ... = y            : h y,
show ∃ x, f x = y, from exists.intro x this
-- END
#+END_SRC

** Defining the Inverse Classically

All the theorems listed in the previous section are found in the Lean
library, and are available to you when you open the function namespace
with =open function=:
#+BEGIN_SRC lean
open function

check comp
check left_inverse
check has_right_inverse
#+END_SRC

Defining inverse functions, however, requires classical reasoning,
which we get by opening the classical namespace:
#+BEGIN_SRC lean
open classical

section
  variables A B : Type
  variable P : A → Prop
  variable R : A → B → Prop

  example : (∀ x, ∃ y, R x y) → ∃ f, ∀ x, R x (f x) :=
  axiom_of_choice

  example (H : ∃ x, P x) : P (some H) :=
  some_spec H
end
#+END_SRC
The axiom of choice tells us that if, for every =x : X=, there is a
=y : Y= satisfying =R x y=, then there is a function =f : X → Y=
which, for every =x= chooses such a =y=. In Lean, this "axiom" is
proved using a classical construction, the =some= function (sometimes
called "the indefinite description operator") which, given that there
is some =x= satisfying =P x=, returns such an =x=. With these
constructions, the inverse function is defined as follows:
#+BEGIN_SRC lean
open classical function

variables {X Y : Type}

noncomputable definition inverse (f : X → Y) (default : X) : Y → X :=
λ y, if H : ∃ x, f x = y then some H else default
#+END_SRC
Lean requires us to acknowledge that the definition is not
computational, since, first, it may not be algorithmically
possible to decide whether or not condition =H= holds, and even if it
does, it may not be algorithmically possible to find a suitable value
of =x=.

Below, the proposition =inverse_of_exists= asserts that =inverse=
meets its specification, and the subsequent theorem shows that if =f=
is injective, then the =inverse= function really is a left inverse.
#+BEGIN_SRC lean
open classical function

variables {X Y : Type}

noncomputable definition inverse (f : X → Y) (default : X) : Y → X :=
λ y, if H : ∃ x, f x = y then some H else default

-- BEGIN
proposition inverse_of_exists (f : X → Y) (default : X) (y : Y) (H : ∃ x, f x = y) :
f (inverse f default y) = y :=
have H1 : inverse f default y = some H, from dif_pos H,
have H2 : f (some H) = y, from some_spec H,
eq.subst (eq.symm H1) H2

theorem is_left_inverse_of_injective (f : X → Y) (default : X) (injf : injective f) :
left_inverse (inverse f default) f :=
let finv := (inverse f default) in
take x,
have H1 : ∃ x', f x' = f x, from exists.intro x rfl,
have H2 : f (finv (f x)) = f x, from inverse_of_exists f default (f x) H1,
show finv (f x) = x, from injf H2
-- END
#+END_SRC

** Functions and Sets in Lean

# we need to introduce relativized quantifiers in an earlier chapter
# on set theory

The Lean library also supports reasoning about the behavior of
functions on subsets of the ambient types. Remember that the library
defines relativized quantifiers as follows:
#+BEGIN_SRC lean
import data.set
open set

variables (X : Type) (A : set X) (P : X → Prop)

example (H : ∀ x, x ∈ A → P x) : ∀₀ x ∈ A, P x := H
example (H : ∃ x, x ∈ A ∧ P x) : ∃₀ x ∈ A, P x := H
#+END_SRC
In the definition of the bounded quantifiers above, the variable =x=
is marked implicit. So, for example, we can apply the hypothesis =H :
∀₀ x ∈ A, P x= as follows:
#+BEGIN_SRC lean
import data.set
open set 

variables (X : Type) (A : set X) (P : X → Prop)

-- BEGIN
example (H : ∀₀ x ∈ A, P x) (x : X) (H1 : x ∈ A) : P x := H H1
-- END
#+END_SRC
The expression =maps_to f A B= asserts that =f= maps elements of the
set =A= to the set =B=:
#+BEGIN_SRC lean
import data.set
open set function

variables X Y : Type
variables (A : set X) (B : set Y)
variable (f : X → Y)

example (H : ∀₀ x ∈ A, f x ∈ B) : maps_to f A B := H
#+END_SRC
The expression =inj_on f A= asserts that =f= is injective on =A=:
#+BEGIN_SRC lean
import data.set
open set function

variables X Y : Type
variable (A : set X)
variable (f : X → Y)

-- BEGIN
example (H : ∀ x₁ x₂, x₁ ∈ A → x₂ ∈ A → f x₁ = f x₂ → x₁ = x₂) : inj_on f A := H
-- END
#+END_SRC
The variables =x₁= and =x₂= are marked implicit in the definition of
=inj_on=, so that the hypothesis is applied as follows:
#+BEGIN_SRC lean
import data.set
open set function

variables X Y : Type
variable (A : set X)
variable (f : X → Y)

-- BEGIN
example (Hinj : inj_on f A) (x₁ x₂ : X) (H1 : x₁ ∈ A) (H2 : x₂ ∈ A)
  (H : f x₁ = f x₂) : x₁ = x₂ :=
Hinj H1 H2 H
-- END
#+END_SRC
The expression =surj_on f A B= asserts that, viewed as a function
defined on elements of =A=, the function =f= is surjective onto the
set =B=:
#+BEGIN_SRC lean
import data.set
open set function

variables X Y : Type
variable (A : set X)
variable (f : X → Y)

-- BEGIN
example (H : ∀ x₁ x₂, x₁ ∈ A → x₂ ∈ A → f x₁ = f x₂ → x₁ = x₂) : inj_on f A := H
-- END
#+END_SRC
It is synonymous with the assertion that =B= is a subset of the image
of =A=, which is written =f ' A=, or, equivalently, =image f A=:
#+BEGIN_SRC lean
import data.set
open set function

variables X Y : Type
variables (A  : set X) (B : set Y)
variable (f : X → Y)

-- BEGIN
example (H : B ⊆ f ' A) : surj_on f A B := H
-- END
#+END_SRC
With these notions in hand, we can prove that the composition of
injective functions is injective. The proof is similar to the one
above, though now we have to be more careful to relativize claims to
=A= and =B=:
#+BEGIN_SRC lean
import data.set
open set function

variables X Y Z : Type
variables (A : set X) (B : set Y)
variables (f : X → Y) (g : Y → Z)

-- BEGIN
theorem inj_on_comp (fAB : maps_to f A B) (Hg : inj_on g B) (Hf: inj_on f A) :
  inj_on (g ∘ f) A :=
take x1 x2 : X,
assume x1A : x1 ∈ A,
assume x2A : x2 ∈ A,
have  fx1B : f x1 ∈ B, from fAB x1A,
have  fx2B : f x2 ∈ B, from fAB x2A,
assume  H1 : g (f x1) = g (f x2),
have    H2 : f x1 = f x2, from Hg fx1B fx2B H1,
show x1 = x2, from Hf x1A x2A H2
-- END
#+END_SRC
We can similarly prove that the composition of surjective functions is surjective:
#+BEGIN_SRC lean
import data.set
open set function

variables X Y Z : Type
variables (A : set X) (B : set Y) (C : set Z)
variables (f : X → Y) (g : Y → Z)

-- BEGIN
theorem surj_on_comp (Hg : surj_on g B C) (Hf: surj_on f A B) :
  surj_on (g ∘ f) A C :=
take z,
assume zc : z ∈ C,
obtain y (H1 : y ∈ B ∧ g y = z), from Hg zc,
obtain x (H2 : x ∈ A ∧ f x = y), from Hf (and.left H1),
show ∃x, x ∈ A ∧ g (f x) = z, from
  exists.intro x
    (and.intro
      (and.left H2)
      (calc
        g (f x) = g y : {and.right H2}
            ... = z   : and.right H1))
-- END
#+END_SRC
The folowing shows that the image of a union is the union of images:
#+BEGIN_SRC lean
import data.set
open set function

variables X Y : Type
variables (A₁ A₂ : set X)
variable (f : X → Y)

-- BEGIN
theorem image_union : f ' (A₁ ∪ A₂) =f ' A₁ ∪ f ' A₂ :=
ext (take y, iff.intro
  (assume H : y ∈ image f (A₁ ∪ A₂),
    obtain x [(xA₁A₂ : x ∈ A₁ ∪ A₂) (fxy : f x = y)], from H,
    or.elim xA₁A₂
      (assume xA₁, or.inl (mem_image xA₁ fxy))
      (assume xA₂, or.inr (mem_image xA₂ fxy)))
  (assume H : y ∈ image f A₁ ∪ image f A₂,
    or.elim H
      (assume yifA₁ : y ∈ image f A₁,
        obtain x [(xA₁ : x ∈ A₁) (fxy : f x = y)], from yifA₁,
        mem_image (or.inl xA₁) fxy)
      (assume yifA₂ : y ∈ image f A₂,
        obtain x [(xA₂ : x ∈ A₂) (fxy : f x = y)], from yifA₂,
        mem_image (or.inr xA₂) fxy)))
-- END
#+END_SRC

# exercises

# composition of relations

# inverse relation
